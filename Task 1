# Import required libraries
!pip install openai pinecone-client pandas numpy tiktoken

import os
import openai
import pinecone
import pandas as pd
import numpy as np
import tiktoken
from typing import List, Dict
from datetime import datetime

class RAGQABot:
    def __init__(self, openai_api_key: str, pinecone_api_key: str, pinecone_environment: str, index_name: str):
        """
        Initialize the RAG QA Bot with necessary API keys and configurations
        """
        # Initialize OpenAI
        openai.api_key = openai_api_key
        
        # Initialize Pinecone
        pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)
        
        # Create or connect to Pinecone index
        if index_name not in pinecone.list_indexes():
            pinecone.create_index(index_name, dimension=1536, metric='cosine')
        
        self.index = pinecone.Index(index_name)
        self.embedding_model = "text-embedding-ada-002"
        self.completion_model = "gpt-4-turbo-preview"
        
    def get_embedding(self, text: str) -> List[float]:
        """
        Get embeddings for the input text using OpenAI's embedding model
        """
        response = openai.Embedding.create(
            input=text,
            model=self.embedding_model
        )
        return response['data'][0]['embedding']
    
    def chunk_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """
        Split text into smaller chunks for processing
        """
        words = text.split()
        chunks = []
        current_chunk = []
        current_size = 0
        
        for word in words:
            current_size += len(word) + 1  # +1 for space
            if current_size > chunk_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_size = len(word)
            else:
                current_chunk.append(word)
                
        if current_chunk:
            chunks.append(' '.join(current_chunk))
            
        return chunks
    
    def upload_documents(self, documents: List[Dict[str, str]]) -> None:
        """
        Process and upload documents to Pinecone
        documents: List of dictionaries with 'text' and 'metadata' keys
        """
        vectors_to_upsert = []
        
        for doc_idx, doc in enumerate(documents):
            chunks = self.chunk_text(doc['text'])
            
            for chunk_idx, chunk in enumerate(chunks):
                embedding = self.get_embedding(chunk)
                
                metadata = doc.get('metadata', {})
                metadata.update({
                    'chunk_idx': chunk_idx,
                    'doc_idx': doc_idx,
                    'text': chunk
                })
                
                vector_id = f"doc_{doc_idx}_chunk_{chunk_idx}"
                vectors_to_upsert.append((vector_id, embedding, metadata))
                
                # Batch upsert every 100 vectors
                if len(vectors_to_upsert) >= 100:
                    self.index.upsert(vectors=vectors_to_upsert)
                    vectors_to_upsert = []
        
        # Upsert any remaining vectors
        if vectors_to_upsert:
            self.index.upsert(vectors=vectors_to_upsert)
    
    def query(self, question: str, top_k: int = 3) -> str:
        """
        Query the RAG system with a question and get an answer
        """
        # Get question embedding
        question_embedding = self.get_embedding(question)
        
        # Query Pinecone for similar contexts
        query_response = self.index.query(
            vector=question_embedding,
            top_k=top_k,
            include_metadata=True
        )
        
        # Extract relevant contexts
        contexts = [match.metadata['text'] for match in query_response.matches]
        
        # Construct prompt
        prompt = f"""Answer the question based on the following contexts. If the answer cannot be found in the contexts, say "I cannot answer this question based on the available information."

Contexts:
{' '.join(contexts)}

Question: {question}

Answer:"""
        
        # Get completion from OpenAI
        response = openai.ChatCompletion.create(
            model=self.completion_model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that answers questions based on the given context."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2
        )
        
        return response.choices[0].message.content

# Example usage and testing code
def test_rag_qa_bot():
    # Initialize the bot (replace with your API keys)
    bot = RAGQABot(
        openai_api_key="your_openai_api_key",
        pinecone_api_key="your_pinecone_api_key",
        pinecone_environment="your_pinecone_environment",
        index_name="qa-bot-test"
    )
    
    # Sample documents
    sample_documents = [
        {
            "text": """Our company, TechCorp, was founded in 2010. We specialize in developing AI-powered solutions 
                      for businesses. Our flagship product, AIAssist, helps companies automate their customer service 
                      operations. We have offices in New York, London, and Singapore.""",
            "metadata": {"source": "company_profile", "date": "2024-01-01"}
        },
        {
            "text": """TechCorp's AIAssist platform features natural language processing, sentiment analysis, and 
                      automated response generation. The system can handle multiple languages and integrates with 
                      popular CRM systems. Pricing starts at $499 per month for basic features.""",
            "metadata": {"source": "product_documentation", "date": "2024-01-01"}
        }
    ]
    
    # Upload documents
    print("Uploading documents...")
    bot.upload_documents(sample_documents)
    
    # Test questions
    test_questions = [
        "When was TechCorp founded?",
        "What are the main features of AIAssist?",
        "How much does AIAssist cost?",
        "What is the company's revenue?" # This should return a "cannot answer" response
    ]
    
    print("\nTesting questions:")
    for question in test_questions:
        print(f"\nQ: {question}")
        answer = bot.query(question)
        print(f"A: {answer}")

# To run the test, uncomment the following line:
# test_rag_qa_bot()
